{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build FAISS + BM25 Index on GPU (Colab/Kaggle)\n",
    "\n",
    "Run this notebook on **Google Colab** (T4 GPU, free tier is enough) or **Kaggle** (P100).\n",
    "\n",
    "## Steps:\n",
    "1. Upload `corpus.zip` to Colab or mount from Drive\n",
    "2. Run all cells\n",
    "3. Download `index.zip` at the end\n",
    "4. Upload it as a **GitHub Release asset** on your repo\n",
    "5. Update the `INDEX_RELEASE_URL` in `Dockerfile` (see README)\n",
    "\n",
    "Expected time: ~5-10 min on T4 GPU for the full corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 1. Install dependencies ──────────────────────────────────────────────────\n",
    "!pip install -q sentence-transformers faiss-gpu rank-bm25 numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 2. Check GPU ─────────────────────────────────────────────────────────────\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 3. Upload and extract corpus ─────────────────────────────────────────────\n",
    "import os, zipfile, json, re, pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Option A: upload corpus.zip manually via Colab file browser\n",
    "# Option B: mount Google Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# CORPUS_ZIP = '/content/drive/MyDrive/corpus.zip'\n",
    "\n",
    "CORPUS_ZIP  = '/content/corpus.zip'   # change if needed\n",
    "CORPUS_DIR  = Path('/content/corpus')\n",
    "INDEX_DIR   = Path('/content/index')\n",
    "INDEX_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "if not CORPUS_DIR.exists():\n",
    "    print(f'Extracting {CORPUS_ZIP}...')\n",
    "    with zipfile.ZipFile(CORPUS_ZIP, 'r') as z:\n",
    "        z.extractall('/content/')\n",
    "    print('Done.')\n",
    "\n",
    "# List files found\n",
    "files = list(CORPUS_DIR.rglob('*.jsonl')) + list(CORPUS_DIR.rglob('*.json'))\n",
    "print(f'Found {len(files)} corpus files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 4. Load protocols ────────────────────────────────────────────────────────\n",
    "def load_protocols(corpus_path: Path) -> list[dict]:\n",
    "    protocols = []\n",
    "    for fpath in sorted(corpus_path.rglob('*.jsonl')):\n",
    "        with open(fpath, encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    protocols.append(json.loads(line))\n",
    "    for fpath in sorted(corpus_path.rglob('*.json')):\n",
    "        with open(fpath, encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            if isinstance(data, list):\n",
    "                protocols.extend(data)\n",
    "            elif isinstance(data, dict):\n",
    "                protocols.append(data)\n",
    "    print(f'Loaded {len(protocols)} protocols')\n",
    "    return protocols\n",
    "\n",
    "protocols = load_protocols(CORPUS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 5. Chunking ──────────────────────────────────────────────────────────────\n",
    "CHUNK_SIZE    = 600   # words\n",
    "CHUNK_OVERLAP = 100\n",
    "\n",
    "# Split on protocol section headers (roman numerals or numbered)\n",
    "SECTION_RE = re.compile(\n",
    "    r'(?:^|\\n)(?=(?:I{1,3}V?|VI{0,3}|[1-9]\\d*)\\.\\s+[А-ЯA-Z])',\n",
    "    re.MULTILINE,\n",
    ")\n",
    "\n",
    "def chunk_by_sections(text: str) -> list[str]:\n",
    "    sections = SECTION_RE.split(text)\n",
    "    sections = [s.strip() for s in sections if s.strip()]\n",
    "    chunks = []\n",
    "    for section in sections:\n",
    "        words = section.split()\n",
    "        if len(words) <= CHUNK_SIZE:\n",
    "            chunks.append(section)\n",
    "        else:\n",
    "            start = 0\n",
    "            while start < len(words):\n",
    "                end = min(start + CHUNK_SIZE, len(words))\n",
    "                chunks.append(' '.join(words[start:end]))\n",
    "                if end == len(words):\n",
    "                    break\n",
    "                start += CHUNK_SIZE - CHUNK_OVERLAP\n",
    "    return chunks\n",
    "\n",
    "def extract_icd(text: str) -> list[str]:\n",
    "    return list(set(re.findall(r'\\b[A-Z]\\d{2}(?:\\.\\d{1,2})?\\b', text)))\n",
    "\n",
    "# Build chunk list\n",
    "all_chunks = []\n",
    "for proto in protocols:\n",
    "    pid   = proto.get('protocol_id', '')\n",
    "    src   = proto.get('source_file', '')\n",
    "    title = proto.get('title', '')\n",
    "    icds  = proto.get('icd_codes', [])\n",
    "    text  = proto.get('text', '')\n",
    "    all_icds = list(set(icds + extract_icd(text)))\n",
    "    for idx, chunk in enumerate(chunk_by_sections(text)):\n",
    "        all_chunks.append({\n",
    "            'protocol_id': pid,\n",
    "            'source_file': src,\n",
    "            'title': title,\n",
    "            'icd_codes': all_icds,\n",
    "            'chunk': chunk,\n",
    "            'chunk_idx': idx,\n",
    "        })\n",
    "\n",
    "print(f'Total chunks: {len(all_chunks)}')\n",
    "print(f'Sample chunk (first 200 chars): {all_chunks[0][\"chunk\"][:200]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 6. Embed with multilingual-e5-small on GPU ───────────────────────────────\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "EMBED_MODEL = 'intfloat/multilingual-e5-small'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Embedding on: {device}')\n",
    "\n",
    "model = SentenceTransformer(EMBED_MODEL, device=device)\n",
    "\n",
    "# E5 requires 'passage: ' prefix for corpus chunks\n",
    "texts = ['passage: ' + c['chunk'] for c in all_chunks]\n",
    "\n",
    "print(f'Embedding {len(texts)} chunks...')\n",
    "embeddings = model.encode(\n",
    "    texts,\n",
    "    batch_size=128,          # GPU can handle larger batches\n",
    "    normalize_embeddings=True,\n",
    "    show_progress_bar=True,\n",
    ")\n",
    "embeddings = np.array(embeddings, dtype='float32')\n",
    "print(f'Embeddings shape: {embeddings.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 7. Build and save FAISS index ────────────────────────────────────────────\n",
    "import faiss\n",
    "\n",
    "dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)   # inner product = cosine on normalized vecs\n",
    "index.add(embeddings)\n",
    "print(f'FAISS index: {index.ntotal} vectors (dim={dim})')\n",
    "\n",
    "faiss.write_index(index, str(INDEX_DIR / 'faiss.index'))\n",
    "print('Saved faiss.index')\n",
    "\n",
    "# Save metadata (parallel to FAISS vectors)\n",
    "with open(INDEX_DIR / 'metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(all_chunks, f)\n",
    "print('Saved metadata.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 8. Build and save BM25 index ─────────────────────────────────────────────\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "def tokenize(text: str) -> list[str]:\n",
    "    text = text.lower()\n",
    "    tokens = re.split(r'\\s+', text)\n",
    "    return [t.strip('.,;:!?()[]') for t in tokens if t.strip('.,;:!?()[]')]\n",
    "\n",
    "print('Building BM25 index...')\n",
    "tokenized_corpus = [tokenize(c['chunk']) for c in all_chunks]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "with open(INDEX_DIR / 'bm25.pkl', 'wb') as f:\n",
    "    pickle.dump({'bm25': bm25, 'chunks': all_chunks}, f)\n",
    "print('Saved bm25.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 9. Quick sanity check ────────────────────────────────────────────────────\n",
    "test_query = 'кашель с мокротой температура боль в грудной клетке'\n",
    "\n",
    "# FAISS\n",
    "q_vec = model.encode(['query: ' + test_query], normalize_embeddings=True)\n",
    "scores, indices = index.search(q_vec.astype('float32'), 3)\n",
    "print('\\n=== FAISS top-3 ===')\n",
    "for score, idx in zip(scores[0], indices[0]):\n",
    "    c = all_chunks[idx]\n",
    "    print(f'  {score:.3f} | {c[\"source_file\"]} | ICD: {c[\"icd_codes\"][:3]}')\n",
    "    print(f'         {c[\"chunk\"][:120]}...')\n",
    "\n",
    "# BM25\n",
    "bm25_scores = bm25.get_scores(tokenize(test_query))\n",
    "top3 = np.argsort(bm25_scores)[::-1][:3]\n",
    "print('\\n=== BM25 top-3 ===')\n",
    "for idx in top3:\n",
    "    c = all_chunks[idx]\n",
    "    print(f'  {bm25_scores[idx]:.3f} | {c[\"source_file\"]} | ICD: {c[\"icd_codes\"][:3]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 10. Package and download ─────────────────────────────────────────────────\n",
    "import shutil\n",
    "\n",
    "output_zip = '/content/index.zip'\n",
    "shutil.make_archive('/content/index', 'zip', '/content/index')\n",
    "\n",
    "# Check size\n",
    "size_mb = os.path.getsize(output_zip) / 1024 / 1024\n",
    "print(f'index.zip size: {size_mb:.1f} MB')\n",
    "\n",
    "# Download in Colab\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(output_zip)\n",
    "    print('Download started!')\n",
    "except ImportError:\n",
    "    print(f'Not in Colab — find the file at: {output_zip}')\n",
    "    print('In Kaggle: go to Output tab to download index.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After downloading `index.zip`\n",
    "\n",
    "1. Go to your GitHub repo → **Releases** → **Draft a new release**\n",
    "2. Tag: `v1.0-index` (or any tag)\n",
    "3. Attach `index.zip` as a release asset\n",
    "4. Publish the release\n",
    "5. Copy the direct download URL of `index.zip`\n",
    "6. Set `INDEX_RELEASE_URL` in your `Dockerfile` (see next cell for the URL format)\n",
    "\n",
    "URL format:\n",
    "```\n",
    "https://github.com/<your-org>/<your-repo>/releases/download/v1.0-index/index.zip\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
